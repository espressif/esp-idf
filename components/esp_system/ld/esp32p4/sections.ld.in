/*
 * SPDX-FileCopyrightText: 2022 Espressif Systems (Shanghai) CO LTD
 *
 * SPDX-License-Identifier: Apache-2.0
 */

#include "sdkconfig.h"
#include "ld.common"

/* Default entry point */
ENTRY(call_start_cpu0);

SECTIONS
{
  /**
   * RTC fast memory holds RTC wake stub code,
   * including from any source file named rtc_wake_stub*.c
   */
  .rtc.text :
  {
    /* Align the start of RTC code region as per PMP granularity
     * this ensures we do not overwrite the permissions for the previous
     * region (ULP mem/RTC reserved) regardless of their end alignment
     */
    ALIGNED_SYMBOL(_esp_pmp_align_size, _rtc_fast_start)
    ALIGNED_SYMBOL(_esp_pmp_align_size, _rtc_text_start)

    arrays[rtc_text]

    mapping[rtc_text]

    /* mutable libs begin */
    mutable[rtc_text]
    /* mutable libs end */

    *rtc_wake_stub*.*(.text .text.*)
    *(.rtc_text_end_test)

    /* Align the end of RTC code region as per PMP granularity */
    . = ALIGN(_esp_pmp_align_size);

    ALIGNED_SYMBOL(4, _rtc_text_end)
  } > lp_ram_seg

  /**
   * This section located in RTC FAST Memory area.
   * It holds data marked with RTC_FAST_ATTR attribute.
   * See the file "esp_attr.h" for more information.
   */
  .rtc.force_fast :
  {
    ALIGNED_SYMBOL(4, _rtc_force_fast_start)

    arrays[rtc_force_fast]

    mapping[rtc_force_fast]

    /* mutable libs begin */
    mutable[rtc_force_fast]
    /* mutable libs end */

    *(.rtc.force_fast .rtc.force_fast.*)

    ALIGNED_SYMBOL(4, _rtc_force_fast_end)
  } > lp_ram_seg

  /**
   * RTC data section holds RTC wake stub
   * data/rodata, including from any source file
   * named rtc_wake_stub*.c and the data marked with
   * RTC_DATA_ATTR, RTC_RODATA_ATTR attributes.
   */
  .rtc.data :
  {
    _rtc_data_start = ABSOLUTE(.);

    arrays[rtc_data]

    mapping[rtc_data]

    /* mutable libs begin */
    mutable[rtc_data]
    /* mutable libs end */

    *rtc_wake_stub*.*(.data .rodata .data.* .rodata.* .srodata.*)

    _rtc_data_end = ABSOLUTE(.);
  } > lp_ram_seg

  /* RTC bss, from any source file named rtc_wake_stub*.c */
  .rtc.bss (NOLOAD) :
  {
    _rtc_bss_start = ABSOLUTE(.);

    *rtc_wake_stub*.*(.bss .bss.* .sbss .sbss.*)
    *rtc_wake_stub*.*(COMMON)

    arrays[rtc_bss]

    mapping[rtc_bss]

    /* mutable libs begin */
    mutable[rtc_bss]
    /* mutable libs end */

    _rtc_bss_end = ABSOLUTE(.);
  } > lp_ram_seg

  /**
   * This section holds data that should not be initialized at power up
   * and will be retained during deep sleep.
   * User data marked with RTC_NOINIT_ATTR will be placed
   * into this section. See the file "esp_attr.h" for more information.
   */
  .rtc_noinit (NOLOAD):
  {
    ALIGNED_SYMBOL(4, _rtc_noinit_start)

    mapping[rtc_noinit]

    *(.rtc_noinit .rtc_noinit.*)

    ALIGNED_SYMBOL(4, _rtc_noinit_end)
  } > lp_ram_seg

  /**
   * This section located in RTC SLOW Memory area.
   * It holds data marked with RTC_SLOW_ATTR attribute.
   * See the file "esp_attr.h" for more information.
   */
  .rtc.force_slow :
  {
    ALIGNED_SYMBOL(4, _rtc_force_slow_start)

    *(.rtc.force_slow .rtc.force_slow.*)

    ALIGNED_SYMBOL(4, _rtc_force_slow_end)
  } > lp_ram_seg

  /**
   * This section holds RTC data that should have fixed addresses.
   * The data are not initialized at power-up and are retained during deep
   * sleep.
   */
  .rtc_reserved (NOLOAD):
  {
    ALIGNED_SYMBOL(4, _rtc_reserved_start)

    KEEP(*(.bootloader_data_rtc_mem .bootloader_data_rtc_mem.*))
    *(.rtc_timer_data_in_rtc_mem .rtc_timer_data_in_rtc_mem.*)

    /**
     * New data can only be added here to ensure existing data are not moved.
     * Because data have adhered to the beginning of the segment and code is relied
     * on it.
     * >> put new data here <<
     */

    _rtc_reserved_end = ABSOLUTE(.);
  } > rtc_reserved_seg

  _rtc_reserved_length = _rtc_reserved_end - _rtc_reserved_start;
  _rtc_ulp_memory_start = _rtc_reserved_start + LENGTH(rtc_reserved_seg);
  ASSERT((_rtc_reserved_length <= LENGTH(rtc_reserved_seg)),
          "RTC reserved segment data does not fit.")

  /* Get size of rtc slow data based on rtc_data_location alias */
  _rtc_slow_length = (ORIGIN(rtc_slow_seg) == ORIGIN(rtc_data_location))
                        ? (_rtc_force_slow_end - _rtc_data_start)
                        : (_rtc_force_slow_end - _rtc_force_slow_start);

  _rtc_fast_length = (ORIGIN(rtc_slow_seg) == ORIGIN(rtc_data_location))
                        ? (_rtc_force_fast_end - _rtc_fast_start)
                        : (_rtc_noinit_end - _rtc_fast_start);

  ASSERT((_rtc_slow_length <= LENGTH(rtc_slow_seg)),
          "RTC_SLOW segment data does not fit.")

  ASSERT((_rtc_fast_length <= LENGTH(rtc_data_seg)),
          "RTC_FAST segment data does not fit.")

  .tcm.text :
  {
    /* Code marked as running out of TCM */
    _tcm_text_start = ABSOLUTE(.);

    arrays[tcm_text]

    mapping[tcm_text]

    /* mutable libs begin */
    mutable[tcm_text]
    /* mutable libs end */

    _tcm_text_end = ABSOLUTE(.);
  } > tcm_idram_seg

  .tcm.data :
  {
    _tcm_data_start = ABSOLUTE(.);

    arrays[tcm_data]

    mapping[tcm_data]

    /* mutable libs begin */
    mutable[tcm_data]
    /* mutable libs end */

    _tcm_data_end = ABSOLUTE(.);
  } > tcm_idram_seg

  .iram0.text :
  {
    _iram_start = ABSOLUTE(.);
    /* Vectors go to start of IRAM */
    ASSERT(ABSOLUTE(.) % 0x40 == 0, "vector address must be 64 byte aligned");
    KEEP(*(.exception_vectors_table.text));
    KEEP(*(.exception_vectors.text));

    /* Code marked as running out of IRAM */
    _iram_text_start = ABSOLUTE(.);

    arrays[iram0_text]

    mapping[iram0_text]

    /* mutable libs begin */
    mutable[iram0_text]
    FAST_REFLASHING_PADDING;
    /* mutable libs end */

  } > sram_low

  /* Marks the end of IRAM code segment */
  .iram0.text_end (NOLOAD) :
  {
    /* Align the end of code region as per PMP region granularity */
    . = ALIGN(_esp_pmp_align_size);

    ALIGNED_SYMBOL(4, _iram_text_end)
  } > sram_low

  .iram0.data :
  {
    ALIGNED_SYMBOL(16, _iram_data_start)

    arrays[iram0_data]

    mapping[iram0_data]

    /* mutable libs begin */
    mutable[iram0_data]
    /* mutable libs end */

    _iram_data_end = ABSOLUTE(.);
  } > sram_low

  .iram0.bss (NOLOAD) :
  {
    ALIGNED_SYMBOL(16, _iram_bss_start)

    arrays[iram0_bss]

    mapping[iram0_bss]

    /* mutable libs begin */
    mutable[iram0_bss]
    /* mutable libs end */

    _iram_bss_end = ABSOLUTE(.);

    ALIGNED_SYMBOL(16, _iram_end)
  } > sram_low

  .dram0.data :
  {
    _data_start_low = ABSOLUTE(.);
    *(.gnu.linkonce.d.*)
    *(.data1)
    __global_pointer$ = . + 0x800;
    *(.sdata)
    *(.sdata.*)
    *(.gnu.linkonce.s.*)
    *(.gnu.linkonce.s2.*)
    *(.jcr)

    arrays[dram0_data]

    mapping[dram0_data]

    /* mutable libs begin */
    mutable[dram0_data]
    FAST_REFLASHING_PADDING;
    /* mutable libs end */

    _data_end_low = ABSOLUTE(.);
  } > sram_low

  .dram1.data :
  {
    _data_start_high = ABSOLUTE(.);

    mapping[dram0_data]

    /* mutable libs begin */
    mutable[dram0_data]
    FAST_REFLASHING_PADDING;
    /* mutable libs end */

    _data_end_high = ABSOLUTE(.);
  } > sram_high

  /**
   * This section holds data that should not be initialized at power up.
   * The section located in Internal SRAM memory region. The macro _NOINIT
   * can be used as attribute to place data into this section.
   * See the "esp_attr.h" file for more information.
   */
  .noinit (NOLOAD):
  {
    ALIGNED_SYMBOL(4, _noinit_start)

    *(.noinit .noinit.*)

    ALIGNED_SYMBOL(4, _noinit_end)
  } > sram_low

  .flash.text :
  {
    _stext = .;
    /**
     * Mark the start of flash.text.
     * This can be used by the MMU driver to maintain the virtual address.
     */
    _instruction_reserved_start = ABSOLUTE(.);
    _text_start = ABSOLUTE(.);

    arrays[flash_text]

    mapping[flash_text]

    /* mutable libs begin */
    mutable[flash_text]
    FAST_REFLASHING_PADDING;
    /* mutable libs end */

    *(.stub)
    *(.gnu.linkonce.t.*)
    *(.gnu.warning)
    *(.irom0.text) /* catch stray ICACHE_RODATA_ATTR */

    /**
     * CPU will try to prefetch up to 16 bytes of of instructions.
     * This means that any configuration (e.g. MMU, PMS) must allow
     * safe access to up to 16 bytes after the last real instruction, add
     * dummy bytes to ensure this
     */
    . += _esp_flash_mmap_prefetch_pad_size;

#if CONFIG_SPIRAM_FETCH_INSTRUCTIONS && CONFIG_SPIRAM_PRE_CONFIGURE_MEMORY_PROTECTION
    /* Align the end of flash text region as per PMP granularity to allow using the
     * page alignment gap created while mapping the flash region into the PSRAM memory.
     */
    . = ALIGN(_esp_pmp_align_size);
#endif // CONFIG_SPIRAM_FETCH_INSTRUCTIONS && CONFIG_SPIRAM_PRE_CONFIGURE_MEMORY_PROTECTION

    _text_end = ABSOLUTE(.);
    /**
     * Mark the flash.text end.
     * This can be used for MMU driver to maintain virtual address.
     */
    _instruction_reserved_end = ABSOLUTE(.);
    _etext = .;

    /**
     * Similar to _iram_start, this symbol goes here so it is
     * resolved by addr2line in preference to the first symbol in
     * the flash.text segment.
     */
    _flash_cache_start = ABSOLUTE(0);
  } > text_seg_low

  /**
   * Dummy section represents the .flash.text section but in default_rodata_seg.
   * Thus, it must have its alignment and (at least) its size.
   */
  .flash_rodata_dummy (NOLOAD):
  {
    _flash_rodata_dummy_start = .;

    . = ALIGN(ALIGNOF(.flash.text)) + SIZEOF(.flash.text);

    /* Add alignment of MMU page size + 0x20 bytes for the mapping header. */
    . = ALIGN(_esp_mmu_page_size) + 0x20;
  } > rodata_seg_low

  .flash.appdesc : ALIGN(0x10)
  {
    /**
     * Mark flash.rodata start.
     * This can be used for mmu driver to maintain virtual address
     */
    _rodata_reserved_start = ABSOLUTE(.);
    _rodata_start = ABSOLUTE(.);

    /* !DO NOT PUT ANYTHING BEFORE THIS! */

    /* Should be the first.  App version info. */
    *(.rodata_desc .rodata_desc.*)
    /* Should be the second. Custom app version info. */
    *(.rodata_custom_desc .rodata_custom_desc.*)

    /**
     * Create an empty gap within this section. Thanks to this, the end of this
     * section will match .flash.rodata's begin address. Thus, both sections
     * will be merged when creating the final bin image.
     */
    . = ALIGN(ALIGNOF(.flash.rodata));
  } > rodata_seg_low
  ASSERT_SECTIONS_GAP(.flash.appdesc, .flash.rodata)

  .flash.rodata : ALIGN(0x10)
  {
    _flash_rodata_start = ABSOLUTE(.);

    arrays[flash_rodata]

    mapping[flash_rodata]

#if CONFIG_LIBC_PICOLIBC
    *(.got .got.plt) /* TODO: GCC-439 */
#endif
    *(.irom1.text) /* catch stray ICACHE_RODATA_ATTR */
    *(.gnu.linkonce.r.*)
    *(.rodata1)
    *(.gcc_except_table .gcc_except_table.*)
    *(.gnu.linkonce.e.*)
    . = ALIGN(ALIGNOF(.flash.init_array));
  } > rodata_seg_low
  ASSERT_SECTIONS_GAP(.flash.rodata, .flash.init_array)

  .flash.init_array :
  {
    /**
     * C++ constructor tables.
     *
     * Excluding crtbegin.o/crtend.o since IDF doesn't use the toolchain crt.
     */
    ALIGNED_SYMBOL(4, __preinit_array_start)
    ALIGNED_SYMBOL(4, __bothinit_array_start)
    KEEP (*(.preinit_array))
    __preinit_array_end = ABSOLUTE(.);
    ALIGNED_SYMBOL(4, __init_array_start)
    KEEP (*(SORT_BY_INIT_PRIORITY(EXCLUDE_FILE (*crtend.* *crtbegin.*) .init_array.*)))
    KEEP (*(EXCLUDE_FILE (*crtend.* *crtbegin.*) .init_array))
    __init_array_end = ABSOLUTE(.);
    __bothinit_array_end = ABSOLUTE(.);

    /* Addresses of memory regions reserved via SOC_RESERVE_MEMORY_REGION() */
    ALIGNED_SYMBOL(4, soc_reserved_memory_region_start)
    KEEP (*(.reserved_memory_address))
    soc_reserved_memory_region_end = ABSOLUTE(.);

    /* System init functions registered via ESP_SYSTEM_INIT_FN */
    ALIGNED_SYMBOL(4, _esp_system_init_fn_array_start)
    KEEP (*(SORT_BY_INIT_PRIORITY(.esp_system_init_fn.*)))
    _esp_system_init_fn_array_end = ABSOLUTE(.);

    /* mutable libs begin */
    mutable[flash_rodata]
    FAST_REFLASHING_PADDING;
    /* mutable libs end */

    _rodata_end = ABSOLUTE(.);
    . = ALIGN(ALIGNOF(SECTION_AFTER_FLASH_RODATA));
  } > rodata_seg_low
  ASSERT_SECTIONS_GAP(.flash.init_array, SECTION_AFTER_FLASH_RODATA)

#if EH_FRAME_LINKING_ENABLED
  .eh_frame_hdr :
  {
    ALIGNED_SYMBOL(4, __eh_frame_hdr)

    KEEP (*(.eh_frame_hdr))

    __eh_frame_hdr_end = ABSOLUTE(.);

    . = ALIGN(ALIGNOF(.eh_frame));
  } > rodata_seg_low
  ASSERT_SECTIONS_GAP(.eh_frame_hdr, .eh_frame)

  .eh_frame :
  {
    ALIGNED_SYMBOL(4, __eh_frame)

    KEEP (*(.eh_frame))
    /**
     * As we are not linking with crtend.o, which includes the CIE terminator
     * (see __FRAME_END__ in libgcc sources), it is manually provided here.
     */
    LONG(0);

    __eh_frame_end = ABSOLUTE(.);

    . = ALIGN(ALIGNOF(.flash.tdata));
  } > rodata_seg_low
  ASSERT_SECTIONS_GAP(.eh_frame, .flash.tdata)
#endif // EH_FRAME_LINKING_ENABLED

  .flash.tdata :
  {
    /* Keep tdata and tbss sections contiguous (no gaps between them).
     * The TLS runtime code calculates offsets assuming these sections are
     * adjacent. Gaps would cause incorrect address calculations, leading
     * to accessing wrong memory.
     *
     * Storing all TLS structures in flash increases binary size, but avoids
     * runtime issues and reduces TLS allocations on the stack.
     */
    /* tdata sections */
    _thread_local_data_start = ABSOLUTE(.);
#if CONFIG_LIBC_PICOLIBC
    _picolibc_reent_stub_start = ABSOLUTE(.);
    KEEP(*(.tdata.errno))
#if CONFIG_LIBC_PICOLIBC_NEWLIB_COMPATIBILITY
    /* Reproduce the public fields from struct _reent. */
    KEEP(*(.tdata.tls_stdin))
    KEEP(*(.tdata.tls_stdout))
    KEEP(*(.tdata.tls_stderr))
#endif // CONFIG_LIBC_PICOLIBC_NEWLIB_COMPATIBILITY
    _picolibc_reent_stub_end = ABSOLUTE(.);
#endif // CONFIG_LIBC_PICOLIBC
    *(.tdata .tdata.* .gnu.linkonce.td.*)
    _thread_local_data_end = ABSOLUTE(.);

    /* tbss sections */
    _thread_local_bss_start = ABSOLUTE(.);
    *(.tbss .tbss.* .gnu.linkonce.tb.*)
    *(.tcommon .tcommon.*)
    _thread_local_bss_end = ABSOLUTE(.);

    . = ALIGN(ALIGNOF(.flash.rodata_noload));
#if CONFIG_SPIRAM_RODATA && CONFIG_SPIRAM_PRE_CONFIGURE_MEMORY_PROTECTION
    /* Align the end of flash rodata region as per PMP granularity to allow using the
     * page alignment gap created while mapping the flash region into the PSRAM memory.
     */
    . = ALIGN(_esp_pmp_align_size);
#endif // CONFIG_SPIRAM_RODATA && CONFIG_SPIRAM_PRE_CONFIGURE_MEMORY_PROTECTION
  } > rodata_seg_low
  ASSERT_SECTIONS_GAP(.flash.tdata, .flash.rodata_noload)
  ASSERT(_thread_local_data_end == _thread_local_bss_start,
         "tdata and tbss must be contiguous.")
  ASSERT_PICOLIBC_REENT_STUB()

  /**
   * This section contains all the rodata that is not used
   * at runtime, helping to avoid an increase in binary size.
   */
  .flash.rodata_noload (NOLOAD) :
  {
    /**
     * This symbol marks the end of flash.rodata. It can be utilized by the MMU
     * driver to maintain the virtual address.
     * NOLOAD rodata may not be included in this section.
     */
    _rodata_reserved_end = .;

    arrays[rodata_noload]

    mapping[rodata_noload]

    /* mutable libs begin */
    mutable[rodata_noload]
    /* mutable libs end */

  } > rodata_seg_low

#if CONFIG_SPIRAM_XIP_FROM_PSRAM
  /**
   * This section is required to skip flash sections, because `extern_ram_seg`
   * and `drom_seg` / `irom_seg` are on the same bus when xip on psram
   */
  .ext_ram.dummy (NOLOAD):
  {
    . = ORIGIN(ext_ram_seg) + (_rodata_reserved_end - _flash_rodata_dummy_start);
    . = ALIGN (_esp_mmu_page_size);
  } > ext_ram_seg
#endif  //CONFIG_SPIRAM_XIP_FROM_PSRAM

#if CONFIG_SPIRAM_ALLOW_BSS_SEG_EXTERNAL_MEMORY
  /* This section holds .ext_ram.bss data, and will be put in PSRAM */
  .ext_ram.bss (NOLOAD) :
  {
    _ext_ram_bss_start = ABSOLUTE(.);

    arrays[extern_ram]

    mapping[extern_ram]

    /* mutable libs begin */
    mutable[extern_ram]
    /* mutable libs end */

    ALIGNED_SYMBOL(4, _ext_ram_bss_end)
  } > ext_ram_seg
#endif  //CONFIG_SPIRAM_ALLOW_BSS_SEG_EXTERNAL_MEMORY

#if CONFIG_SPIRAM_ALLOW_NOINIT_SEG_EXTERNAL_MEMORY
  /**
   * This section holds data that won't be initialised when startup.
   * This section locates in External RAM region.
   */
  .ext_ram_noinit (NOLOAD) :
  {
    _ext_ram_noinit_start = ABSOLUTE(.);

    *(.ext_ram_noinit*)

    ALIGNED_SYMBOL(4, _ext_ram_noinit_end)
  } > ext_ram_seg
#endif //CONFIG_SPIRAM_ALLOW_NOINIT_SEG_EXTERNAL_MEMORY

  .dram0.bss (NOLOAD) :
  {
    ALIGNED_SYMBOL(4, _bss_start_low)

    /**
     * ldgen places all bss-related data to mapping[dram0_bss]
     * (See components/esp_system/app.lf).
     */
    arrays[dram0_bss]

    mapping[dram0_bss]

    /* mutable libs begin */
    mutable[dram0_bss]
    /* mutable libs end */

    ALIGNED_SYMBOL(4, _bss_end_low)
  } > sram_low

  .dram1.bss (NOLOAD) :
  {
    ALIGNED_SYMBOL(4, _bss_start_high)

    /**
     * ldgen places all bss-related data to mapping[dram0_bss]
     * (See components/esp_system/app.lf).
     */
    mapping[dram0_bss]

    /* mutable libs begin */
    mutable[dram0_bss]
    /* mutable libs end */

    ALIGNED_SYMBOL(4, _bss_end_high)
  } > sram_high

  /* Marks the end of data, bss and possibly rodata */
  .dram0.heap_start_low (NOLOAD) :
  {
    ALIGNED_SYMBOL(16, _heap_start_low)
  } > sram_low

    /* Marks the end of data, bss and possibly rodata */
  .dram1.heap_start_high (NOLOAD) :
  {
    ALIGNED_SYMBOL(16, _heap_start_high)
  } > sram_high

#include "elf_misc.ld.in"
}
